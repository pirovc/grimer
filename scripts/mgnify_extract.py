#!/usr/bin/env python3
import pandas as pd
import gzip
import os
import pickle
import argparse


def main():
    version = "1.0.0"

    parser = argparse.ArgumentParser(description='mgnify_extract')
    parser.add_argument('-f', '--input-folder', required=True, type=str, help="Folder with files generated by mgnify_download.py")
    parser.add_argument('-t', '--top-taxa', default=10, type=int, help="Top taxa to use for each study. 0 for everything. Default 10.")
    parser.add_argument('-o', '--output-taxa-counts', type=str, default="taxa_counts.tsv")
    parser.add_argument('-b', '--output-biome-counts', type=str, default="")

    parser.add_argument('-v', '--version', action='version', version='%(prog)s ' + version)
    args = parser.parse_args()

    acc_files = select_files(args.input_folder)
    print("Number of files found: ", len(acc_files))

    rank_id_name = {0: "superkingdom",
                    1: "kingdom",
                    2: "phylum",
                    3:"class",
                    4:"order",
                    5:"family",
                    6:"genus",
                    7:"species"}
    taxa_biome = {}
    no_biome = []
    biome_count = {}

    for study_accession, study_table in acc_files.items():
        print(study_accession)

        study_file = args.input_folder + "/" + study_accession + ".pkl.gz"

        if os.path.isfile(study_file):
            with gzip.open(study_file) as f:
                study = pickle.load(f)
        else:
            no_biome.append(study_accession)
            continue
        
        study_biomes = []
        if 'biomes' in study['relationships']:
            for b in study['relationships']['biomes']['data']:
                biome = b['id']
                if biome not in taxa_biome: taxa_biome[biome] = {}
                if biome not in biome_count: biome_count[biome] = 0
                study_biomes.append(biome)
                biome_count[biome]+=1
        else:
            no_biome.append(study_accession)
            continue

        pipeline_version = float(study_table[-10:-7])

        t = pd.read_table(study_table)

        sample_col = '#SampleID'
        if sample_col not in t.columns:
            # older files have a different header
            sample_col = 'taxonomy'

        # expand ranks in columns
        ranks = t[sample_col].str.split(';', expand=True)

        # replace empty for unclassified
        ranks.replace(regex={r'^.+__$': 'unclassified'}, inplace=True)

        # Replace "s__" at the beggining and _ for " "
        ranks.replace(regex={r'^.+__': '', '_': ' '}, inplace=True)

        # Pipeline <= 3.0 reports only species specific name, need to merge
        if pipeline_version <= 3.0:
            if 6 in ranks and 7 in ranks:
                #print(ranks)
                # Replace unclassified with None  
                ranks[6] = ranks[6].replace("unclassified", None)
                ranks[7] = ranks[7].replace("unclassified", None)
                ranks["species"] = ranks[6] + " " + ranks[7]
                ranks.drop(columns=7, inplace=True)
                ranks.rename(columns={"species": 7}, inplace=True)
                #print(ranks)

        t = pd.concat([ranks, t], axis=1)
        t.drop(columns=sample_col, inplace=True)
        top_taxa_rank = {}

        for r in range(ranks.shape[1]):
            rank_table = t.groupby([r]).sum().T
            # Do not count for unclassified taxa
            if "unclassified" in rank_table.columns:
                rank_table.drop(columns="unclassified", inplace=True)
            if "Unclassified" in rank_table.columns:
                rank_table.drop(columns="Unclassified", inplace=True)

            max_count = rank_table.max().max()
            n_samples = rank_table.shape[0]
            avg_perc_taxa = ((rank_table/max_count).sum(axis=0) / n_samples).sort_values(ascending=False)
            if args.top_taxa:
                top_taxa_rank[r] = avg_perc_taxa.iloc[:args.top_taxa].index.to_list()
            else:
                top_taxa_rank[r] = avg_perc_taxa.index.to_list()

        # Study can have multiple biomes, for each count
        for biome in study_biomes:
            for rank, taxa in top_taxa_rank.items():
                r = rank_id_name[rank]
                for t in taxa: 
                    if (r,t) not in taxa_biome[biome]: taxa_biome[biome][(r,t)] = 0
                    taxa_biome[biome][(r,t)]+=1

    biomes_df = pd.DataFrame.from_dict(taxa_biome).T
    stacked = pd.DataFrame(biomes_df.T.stack(), columns=["count"])

    stacked.to_csv(args.output_taxa_counts, sep="\t", header=None)

    if no_biome:
        print("Skipped " + str(len(no_biome)) + " files without defined file/biome")

    if args.output_biome_counts:
        with open(args.output_biome_counts, "w") as bf:
            for biome, cnt in biome_count.items():
                print(biome, cnt, sep="\t", file=bf)


def select_files(input_folder):
    """
    Return a dict with {accession: taxonomy abundance file}
    One file per accession (biggest in size)
    """
    tax_ab_files = {}
    for file in os.listdir(input_folder):
        ffile = input_folder + "/" + file
        bname = os.path.basename(file)

        if "taxonomy_abundances" in file:
            accession = bname.split("_")[0]
            # Add file and filesize
            if accession not in tax_ab_files: tax_ab_files[accession] = tuple(["", 0])
            fsize = os.path.getsize(ffile)
            if fsize > tax_ab_files[accession][1]:
                tax_ab_files[accession] = (ffile, fsize)

    return {k: v[0] for k, v in tax_ab_files.items()}


if __name__ == "__main__":
    main()
